{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Movie Recommendation System\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://movielens.org/images/site/main-screen.png\" width = 600>\n",
    "</p>\n",
    "\n",
    "Understanding user preferences is essential for recommendation systems in the entertainment industry, especially in movies. The MovieLens dataset is a rich source of user behavior data, including movie ratings, that can help explore trends and preferences. Hashing and clustering techniques can be implemented to analyze and segment users to provide personalized recommendations.\n",
    "\n",
    "You and your team were hired to help hired to work on a recommendation system. Your tasks are to apply hashing techniques to improve data retrieval speed and to use clustering to group users by similar movie preferences. \n",
    "\n",
    "---\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "1. **!!! Read the entire homework before coding anything!!!**\n",
    "2. *My solution is not better than yours, and yours is not better than mine*. In any data analysis task, there **is no** unique way to answer. For this reason, it is crucial (**necessary and mandatory**) that you describe every decision you make and all your steps.\n",
    "3. Once solving an exercise, comments about the obtained results are **mandatory**. We are not always explicit about where to focus your comments, but we will always want brief sentences about your discoveries.\n",
    "4. We encourage using chatGPT (Claude AI, Gemini, Perplexity, or any other Large Language Models (LLM) chatbot tool) to help you solve your homework, but we expect you to learn how to use them responsibly. **Using such tools when not explicitly allowed will be considered plagiarism and strictly prohibited**.\n",
    "\n",
    "Now that it is all well settled, let's get on with it!\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recommendation System with LSH\n",
    "\n",
    "In this section, you will implement a recommendation system by identifying users with similar preferences and suggesting movies based on their behavior. \n",
    "Specifically, you will implement your version of the [**LSH algorithm**](https://www.learndatasci.com/tutorials/building-recommendation-engine-locality-sensitive-hashing-lsh-python/).\n",
    "\n",
    "### 1.1 Data Preparation\n",
    "\n",
    "Download the MovieLens dataset from [here](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset?select=rating.csv). After downloading, explore the dataset to understand the structure and identify any preprocessing steps needed.\n",
    "\n",
    "### 1.2 Minhash Signatures\n",
    "\n",
    "Using the `userId` and `movieId` columns, implement your own MinHash function. This function will hash each user's watched movie list, creating a representation that allows for quick comparisons of user similarities.\n",
    "\n",
    "- **Important:** Implement your MinHash function from scratch—**do not use any pre-built hash functions.**\n",
    "- Use your MinHash function to generate signature vectors for each user based on their rated movies.\n",
    "- Experiment with different hash functions and threshold values to find the most effective configurations. Report these results.\n",
    "- Read the class materials and, if necessary, conduct an internet search. The description of hash functions in the [book](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf) may be helpful as a reference.\n",
    "\n",
    "### 1.3 Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "Now that you have generated MinHash user signatures, apply Locality-Sensitive Hashing (LSH) to cluster similar users.\n",
    "\n",
    "1. **Bucket Creation:** For each user, divide the MinHash signature into bands and hash each band to form buckets. Users with similar bands should fall into the same buckets.\n",
    "   - **Debugging Tip:** After creating buckets, check a few bucket contents to verify that multiple users are being grouped in the same buckets.\n",
    "   \n",
    "2. **Query:** For a given user, identify the **two most similar users** based on their bucket placement. If a user doesn’t have any similar users in their bucket, adjust the parameters until similar users are found.\n",
    "\n",
    "3. **Movie Recommendation Logic:**\n",
    "   - If both similar users have rated a movie, recommend this movie based on the **average rating**.\n",
    "   - If there are no commonly rated movies, recommend the top-rated movies of the most similar user.\n",
    "\n",
    "4. **Final Recommendation:** Provide **at most five movies** to the user. \n",
    "\n",
    "Example recommendation logic for a user:\n",
    "\n",
    "| User | Movie Title         | Rating |\n",
    "|------|----------------------|--------|\n",
    "| A    | Inception           | 4.5    |\n",
    "| A    | Titanic             | 4.2    |\n",
    "| A    | Avatar              | 2.8    |\n",
    "| B    | Inception           | 4.6    |\n",
    "| B    | The Matrix          | 3.9    |\n",
    "| B    | Toy Story           | 4.7    |\n",
    "| C    | Titanic             | 3.8    |\n",
    "| C    | Avatar              | 4.3    |\n",
    "| C    | Shrek               | 4.1    |\n",
    "\n",
    "If User A and User B are identified as the two most similar users to User X, the recommended movies would be:\n",
    "1. **Common Movies:** \"Inception\" (average rating: 4.55).\n",
    "2. **Top-rated from Most Similar User:** \"Toy Story\" (4.7) from User B and \"Titanic\" (4.2) from User A.\n",
    "3. If fewer than 5 movies are found, complete the list using other high-rated movies by the most similar users.\n",
    "\n",
    "## 2. Grouping Movies Together! \n",
    "In this section, you will explore clustering algorithms to group the movies you have based on specific features you choose to consider for them.\n",
    "\n",
    "### 2.1 Feature Engineering \n",
    "As you know, the dataset provided isn’t particularly clean or well-structured to represent the features of the movies. Therefore, your first step is to create a more suitable set of attributes (variables, features, covariates) to represent the movies based on the available information. Here are some variables or features you might consider for clustering:\n",
    "\n",
    "1. ```movieid``` id of each movie \n",
    "2. ```genres``` list of genres attached to the movie (given that a movie may have several genres, it’s essential to devise a method to accurately represent the genres for each movie)\n",
    "3. ```ratings_avg``` the average ratings provided by users for the movie\n",
    "4. ```relevant_genome_tag``` the most relevant tag to the movie given in the genome set\n",
    "5. ```common_user_tag``` the most common tag given to the movie by the users \n",
    "\n",
    "In addition to the above features, include **at least three additional** features for clustering.\n",
    "\n",
    "__Note__: If you have accurately identified and applied the methods for representing the features, you should have __more than eight features__! How could this happen? Take a moment to think about it.\n",
    "\n",
    "### 2.2 Choose your features (variables)!\n",
    "With multiple features available for the movies, you need to consider the following two questions: 1. Should you normalize the data or leave it as is? 2. Should you include all these features, or can you reduce the dimensionality of the data?\n",
    "\n",
    "1. What is the importance of normalizing the data in your analysis, and how does it impact the effectiveness of the clustering algorithms you plan to use?\n",
    "2. If you find that normalizing the values is beneficial, please proceed to normalize the data. To simplify this task, refer to the [scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html) package for tools and functions that facilitate data normalization.\n",
    "3. Could you provide some insights on dimensionality reduction? What techniques would be effective for reducing the number of features in the dataset, and why might this be beneficial for the analysis?\n",
    "4. If you believe dimensionality reduction would be advantageous, please select a method to reduce the dimensionality of the data.\n",
    "\n",
    "### 2.3 Clustering \n",
    "Now that you have prepared the data, you can create the clusters.\n",
    "\n",
    "1. How can you determine the optimal number of clusters for your data? Please use at least two methods and provide their results.\n",
    "2. Implement the K-means clustering algorithm (not K-means++) through MapReduce. We request that you develop the algorithm from scratch based on what you've learned in class and run the algorithm on your data.\n",
    "3. Implement the K-means++ algorithm from scratch and apply it to your data. Do you notice any differences between the results obtained using random initialization and those achieved with K-means++? Please explain your observations and discuss why these differences might occur.\n",
    "4. Ask an LLM (ChatGPT, Claude AI, Gemini, Perplexity, etc.) to recommend another clustering algorithm. Use that LLM to describe the workings of the algorithm, as well as its advantages and disadvantages compared to K-means and K-means++. Additionally, ask to implement the algorithm for you or utilize an existing version from a package. Apply that algorithm to your data and explain any differences you observe in the results compared to those obtained previously.\n",
    "\n",
    " ### 2.4 Best Algorithm\n",
    "Clustering helps identify natural groupings within data, but no single algorithm works best for every dataset. In this section, you’ll learn how to choose the most suitable clustering method based on your data’s unique characteristics. By analyzing patterns and comparing results, you’ll uncover which algorithm provides the most meaningful insights and clusters.\n",
    "\n",
    "\n",
    "1. Set the number of clusters to the optimal number $k_{opt}$ based on any of the methods previously.\n",
    "2. Select three distinct metrics to assess the quality of the clusters. Describe each metric in detail, including the specific aspects they evaluate to determine the effectiveness of the clustering model.\n",
    "3. Apply the three clustering algorithms used in the prior section to partition the data into $k_{opt}$ clusters. Then, evaluate each model's clustering quality using the selected metrics. Summarize your findings by comparing the results of each algorithm based on the metric evaluations.\n",
    "\n",
    "## 3. Bonus Question\n",
    "K-means is an iterative algorithm, meaning that with each iteration, it refines the clusters by adjusting them based on the distance of each data point relative to the center of each cluster. This process continues until it reaches a point of convergence or hits a set limit on the number of iterations. You might want to track the progress of forming your clusters.\n",
    "\n",
    "1. Select two variables* from your instances to display them on a 2D plot. Then, illustrate the progression of the clusters as they change at each iteration. We expect a plot for each iteration, displaying the instances and the clusters they belong to. Select the two features that most effectively separate visual instances belonging to different clusters. Explain the method you used to determine these features.\n",
    "\n",
    "__*Note:__ Depending on the variables you want to use for clustering, whether they are the original movie features or the components derived from PCA, you may select two features/components that best help to visually display the clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HP\\ADM_HW4\\ADM-HM4\\genome_scores.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId  tagId  relevance\n",
      "0        1      1    0.02500\n",
      "1        1      2    0.02500\n",
      "2        1      3    0.05775\n",
      "3        1      4    0.09675\n",
      "4        1      5    0.14675\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11709768 entries, 0 to 11709767\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   movieId    int64  \n",
      " 1   tagId      int64  \n",
      " 2   relevance  float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 268.0 MB\n",
      "None\n",
      "            movieId         tagId     relevance\n",
      "count  1.170977e+07  1.170977e+07  1.170977e+07\n",
      "mean   2.584297e+04  5.645000e+02  1.164833e-01\n",
      "std    3.467615e+04  3.256254e+02  1.542463e-01\n",
      "min    1.000000e+00  1.000000e+00  2.500000e-04\n",
      "25%    2.926000e+03  2.827500e+02  2.425000e-02\n",
      "50%    6.017000e+03  5.645000e+02  5.650000e-02\n",
      "75%    4.606200e+04  8.462500e+02  1.415000e-01\n",
      "max    1.311700e+05  1.128000e+03  1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
